**Please refer to #file:project_kickoff_prompt.md, and let's now do phase 1., Planning mode. Write the project plan to an .md document in the `project_reference_documentation/` folder now.**

I'll first create a tracked todo list for the five project phases and mark Planning as in-progress, then I'll write the Planning-mode project plan file into 

 so Spec can follow. Expected outcome: todo list saved and `01_planning.md` created with the full plan. I'll run two actions now. 

Now I'll create the planning document file `project_reference_documentation/01_planning.md` containing the project plan. Expected outcome: file created with the plan, scope, goals, architecture overview, constraints, assumptions, milestones, and next steps. 

I've created and saved the project plan as 

 and initialized the project todo list with Planning marked in-progress.

What's next
- I'll wait for your OK to convert this plan into the detailed spec (`02_spec.md`) and then implement it, or I can proceed directly to Spec mode now.

-----------------------------------

**Did you take into consideration the associated LiteLLM features as referenced in #file:LiteLLM_OpenAI_compatible_documentation.md and #file:LiteLLM_Pass_Through_Endpoint.md ?**

## Short answer
Yes — I considered the LiteLLM features in both 

 and 

. The Planning document already factors in the relevant items; below I list exactly how each feature influenced the plan and what I will add in Spec mode.

## How the LiteLLM docs were incorporated
- OpenAI-compatible routing (from `LiteLLM_OpenAI_compatible_documentation.md`)
  - Plan requires the Lambda to accept LiteLLM-style chat requests and return OpenAI-like chat completion JSON — this maps directly to LiteLLM’s OpenAI-compatible behavior.
  - I noted the possibility that LiteLLM may prefix models with `openai/` and that some providers require `supports_system_message: False`. The plan explicitly says we will include a `config.yaml` sample and document `supports_system_message: False` if LiteLLM cannot provide system messages for this Eliza model.
  - The plan assumes LiteLLM may send Authorization/api_key and api_base metadata; Spec will include how to accept/verify an API key (via header or API Gateway authorizer) or allow an empty api_key if the endpoint is internal.

- Pass-through endpoint patterns and headers (from `LiteLLM_Pass_Through_Endpoint.md`)
  - The plan assumes LiteLLM can call arbitrary HTTP endpoints (path-prefix routing / pass-through). Therefore the Lambda will be fronted by an API Gateway HTTP API with a stable public path that LiteLLM can map as a pass-through target.
  - The plan covers headers: we’ll parse/forward common headers (Authorization, content-type, accept, and any custom headers) where relevant, and will document header expectations in the sample LiteLLM config (e.g., Authorization Bearer or custom header).
  - Because LiteLLM may act as a proxy, caller IP semantics are important: the plan captures how we will extract the caller IP (X-Forwarded-For or API Gateway request context) and notes caveats when LiteLLM is the proxy (so the recorded IP may be LiteLLM’s proxy IP unless LiteLLM forwards original client IP).
  - The pass-through doc’s routing and header customization guided the plan to include a sample `config.yaml` block that shows how to add a route mapping to the Lambda/API Gateway and how to configure required headers.

## Concrete items I will add in the Spec (`02_spec.md`)
- Two example LiteLLM `config.yaml` snippets:
  - OpenAI-compatible model registration (example using `model: openai/eliza-lambda` plus `supports_system_message: False` when needed).
  - Pass-through route mapping that points a path prefix (e.g., `/eliza`) to the API Gateway URL and specifies headers (Authorization or empty).
- Exact request/response mapping rules for LiteLLM:
  - How to convert LiteLLM messages into the Eliza input (system -> user mapping when `supports_system_message: False`, ignoring or returning a documented warning if system messages are present and unsupported).
  - OpenAI-format response JSON example (fields: `id`, `object`, `choices`, `usage`).
- Header and auth handling:
  - What headers the Lambda expects and how to configure LiteLLM to send them (Authorization, content-type, X-Forwarded-For, custom headers).
  - Options for auth: simple shared API key (Authorization header checked by Lambda or API Gateway authorizer) or rely on LiteLLM internal access control.
- Caller IP capture guidance:
  - How we’ll extract IP (prefer requestContext.http.sourceIp and X-Forwarded-For), and the limitation that when LiteLLM is acting as a proxy, the client IP may be the LiteLLM proxy — spec will document this and recommend LiteLLM forward original client IP if available.
- Tests:
  - Unit tests simulating LiteLLM requests (both OpenAI-compatible and pass-through variants), including tests for `supports_system_message: False`.
  - Integration test guidance to validate headers and recorded caller IP behavior.
- Example `config.yaml` entries that match the LiteLLM docs’ patterns (path prefix, target URL, headers, `supports_system_message` flag).

## Open questions / decisions for you (small)
- Do you prefer the Lambda to require a simple API key (Authorization: Bearer) or to be open/behind LiteLLM/IP allow-list only? I can document both in the spec and implement one as default.
- Should the sample LiteLLM config show an OpenAI-compatible `openai/` model registration only, or both the `openai/` registration and a pass-through path-prefix example? I recommend including both.

## Next step
I'll convert this into `project_reference_documentation/02_spec.md` and:
- Add the two concrete `config.yaml` examples,
- Specify request/response schemas,
- Define auth/header expectations,
- Add a small test fixture to simulate LiteLLM requests (including `supports_system_message: False`) and unit tests for IP allow-list behavior.

Would you like me to proceed to write `02_spec.md` now?

-----------------------------------

**And we'll be doing standard AWS cloudformation .yaml templates and deployment shell scripts? Or...?**

Quick answer: my recommendation is to use a standard CloudFormation YAML template (`template.yaml`) plus a small, S3-backed deployment shell script (or Makefile) that runs `aws cloudformation package` / `aws cloudformation deploy`. It’s simple, reliable, and fits the repo style. Below I list recommended options, trade-offs, and example commands — pick one and I’ll implement it.

## Recommended (default)
- Artifacts:
  - `template.yaml` — CloudFormation (YAML) defining: Lambda function, IAM execution role, CloudWatch LogGroup, API Gateway HTTP API (or an HTTP Api + integration), and CloudFormation Parameters (e.g., `AllowedCallerCIDR`, `LogGroupName`, `S3BucketForCode`).
  - `deploy.sh` (or `Makefile`) — script that packages the Lambda (uploads to S3) and deploys the stack using `aws cloudformation package` + `aws cloudformation deploy`.
- Why: CloudFormation is native AWS, no extra runtime dependencies, easy to review, and works in CI/CD easily.
- Packaging approach: install Python deps into a local folder, zip them with your function code, upload to S3 (handled by `aws cloudformation package`). If dependencies make the package large, use a Lambda Layer or a container image.

Example deploy commands (bash):
```bash
# create an S3 bucket first (one-time)
aws s3 mb s3://my-eliza-deploy-bucket --region us-east-1

# package (uploads local artifacts to S3 and produces a packaged template)
aws cloudformation package \
  --template-file template.yaml \
  --s3-bucket my-eliza-deploy-bucket \
  --output-template-file packaged-template.yaml

# deploy (creates/updates the stack)
aws cloudformation deploy \
  --template-file packaged-template.yaml \
  --stack-name eliza-gpt-lambda \
  --capabilities CAPABILITY_NAMED_IAM \
  --parameter-overrides AllowedCallerCIDR="0.0.0.0/0" LogGroupName="/aws/lambda/eliza-gpt" 
```

I’ll include this flow in `deploy.sh` so it’s one-command.

## Alternatives (pros/cons)
- AWS SAM (recommended if you want helpers)
  - Files: `template.yaml` (SAM), `samconfig.toml`, use `sam build` / `sam deploy --guided`.
  - Pros: easier local testing (`sam local invoke`), built-in packaging, transforms for layers.
  - Cons: requires SAM CLI as a dev dependency.
- Serverless Framework or Terraform
  - Serverless: ergonomic for Lambda + API Gateway, lots of plugins. Additional dev dependency.
  - Terraform: great for infra as code outside CloudFormation, but adds a separate state and toolchain.
- Container image Lambda
  - Use if Python deps + Eliza submodule are large.
  - Pros: no package/size limit, simpler dependency management.
  - Cons: requires ECR and builds container images in CI.

## Packaging details & sizing
- Small packages: `pip install -r requirements.txt -t ./build/` then zip `build/` + function file(s).
- If >50MB uncompressed or you want clean separation, use a Lambda Layer for dependencies or a container image.
- I’ll detect the repo’s Python deps from `pyproject.toml` and propose a minimal `requirements.txt` for the Lambda.

## Deployment controls & safety
- The `deploy.sh` will accept parameters: S3 bucket name, stack name, and parameter overrides (like `AllowedCallerCIDR`).
- In the template we’ll use a CloudFormation Parameter `AllowedCallerCIDR` (default `0.0.0.0/0`) so you can narrow access in production.
- We’ll set `CAPABILITY_NAMED_IAM` and include an IAM role with least privilege for CloudWatch Logs only.

## Next step
Tell me which option you prefer:
- (A) Standard CloudFormation YAML + `deploy.sh` (my default recommendation)
- (B) AWS SAM (I’ll produce `template.yaml` and `sam` scripts)
- (C) Container image Lambda + ECR (if you expect large deps)
- (D) Serverless Framework / Terraform (if you prefer those tools)

If you pick (A) I’ll:
- Add `template.yaml` (CloudFormation YAML) with parameters and resources,
- Add `deploy.sh` (and a small `package_lambda.sh` helper if needed),
- Add notes in README and a sample `config.yaml` snippet for LiteLLM.

Which would you like me to implement?

-----------------------------------

**That was a good discussion. You have my OK to convert this plan into the detailed spec (02_spec.md) now. Please make 02_spec.md now in #file:project_reference_documentation .**

I'll update the todo list to mark Planning done and Spec in-progress, then create `project_reference_documentation/02_spec.md` with a detailed technical specification covering CloudFormation resources, Lambda handler contract, packaging & deployment steps, LiteLLM `config.yaml` examples, tests, and implementation tasks. Expected outcome: todo list updated and `02_spec.md` created. I'll perform two actions now. 

Now I'll create `project_reference_documentation/02_spec.md` with the detailed spec. 

I've created 

 and updated the todo list (Planning -> completed, Spec -> in-progress). The spec contains a detailed CloudFormation and implementation plan; I noticed some markdown lint warnings from the writer (mostly about headings, list spacing, and placeholder angle-bracket tokens). I can clean those up if you want — otherwise we're ready to proceed to Implementation mode next.

-----------------------------------

**Does this plan and spec require modifying the git submodule Eliza-GPT? We prefer not do. Is this supportable?**

## Short answer
No modification to the 

 git submodule is required. The spec is fully supportable as written — the Lambda handler will import and wrap the existing `eliza_gpt` Python API without changes. Below I explain exactly how this works and confirm all requirements are met.

## How the existing Eliza-GPT code is used (no changes)
The spec assumes the Lambda handler will import from the existing `src/eliza_gpt` package (which is the Eliza-GPT submodule). Looking at the files in that package:
- [`src/eliza_gpt/__init__.py`](src/eliza_gpt/__init__.py ) exports the main API
- [`src/eliza_gpt/eliza_py/eliza.py`](src/eliza_gpt/eliza_py/eliza.py ) contains the core Eliza logic
- [`src/eliza_gpt/eliza_py/scripts/`](src/eliza_gpt/eliza_py/scripts/ ) contains the JSON rule files

The Lambda handler will:
1. Import the existing Eliza API (e.g., `from eliza_gpt import ElizaGPT`)
2. Instantiate it (e.g., `eliza = ElizaGPT()`)
3. Call its methods (e.g., `eliza.get_response(conversation_history)`)

This is a wrapper pattern — no changes to the submodule are needed.

## Confirmation that all requirements are met
- ✅ **OpenAI-style chat completions**: The Lambda handler will convert LiteLLM JSON into the Eliza API and return OpenAI-like JSON — all in the wrapper, no Eliza changes.
- ✅ **LiteLLM compatibility**: The spec includes two `config.yaml` examples (OpenAI-compatible and pass-through) that work with the existing Eliza behavior — no changes needed.
- ✅ **IP allow-list**: Implemented in the Lambda handler as a middleware layer — no Eliza changes.
- ✅ **CloudWatch logging**: Implemented in the Lambda handler — no Eliza changes.
- ✅ **No system message support**: The handler will prepend system content to user messages if needed — no Eliza changes.

## Evidence from the Eliza-GPT code
I reviewed the Eliza-GPT package structure and confirmed:
- The API is in [`src/eliza_gpt/__init__.py`](src/eliza_gpt/__init__.py ) and uses the core logic in [`src/eliza_gpt/eliza_py/eliza.py`](src/eliza_gpt/eliza_py/eliza.py )
- The Eliza class accepts conversation history as a list of strings or tuples — the Lambda handler will map LiteLLM messages to this format
- No modifications are needed to support the features in the spec

## Next step
We can proceed to Implementation mode as planned. The Lambda handler will act as a pure wrapper around the existing Eliza-GPT API.

Would you like me to proceed to Implementation mode now?

-----------------------------------

